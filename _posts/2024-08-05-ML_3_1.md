layout: post
title:  "혼공 ML 3-1장"
published: true
date:   2024-08-05 02:50:00
categories: ML(MachineLearning)
permalink: /ml/3-1

# k-최근접 이웃 회귀
- 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하여, 타깃값을 결정하는 회귀

## 결정계수(R2)

- '얼마나 모델이 잘 예측했는가?'를 수치로 나타냄
$$
R^2 = 1-(타깃 - 예측)^2/(타깃-평균)^2
$$

- score()fmf 활용하여 구할 수 있음

```python

#mean_absolute_error: 타깃과 예측긔 절댓값 오차를 평균하여 변환

from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
```

## 과대적합 vs 과소적합

1. 과대적합
- 훈련세트에서 점수가 좋았는데 테스트 세트에서 점수가 굉장히 좋지 않은 경우   

- 즉 훈련세트에만 잘 맞는 모델

2. 과소적합
- 훈련세트 보다 테스트 세트의 점수가 높거나 두 점수가 모두 낮은 경우   

- 모델이 너무 단순하여 훈련세트에 적절히 훈련되지 않은 경우

- 훈련세트와 테스트 세트의 크기가 매우 작기 때문에 발생할 수 있음

> k-최근접 이웃 회귀에서 모델을 복잡하게 만드는 방법은 이웃의 개수 k를 줄이는 것

```python
knr.n_neighbors=3

knr.fit(train_input, train_target)
print(knr.score(train_input,train_target))
```

---
## Numpy 배열 바꾸기

```python

test_array = ([1,2,3,4])
print(test_array.shape)

(4,)

#배열 쿠기 바꾸기(reshape)
test_array=test_array.reshape(2,2)

print(test_array.shape)
(2,2)

#배열의 크기에서 -1을 지정할 때
# 나머지 원소 개수로 모두 채우라는 의미

test_Array = test_array.reshape(-1,4)

print(test_array.reshape)
(1,4)
```