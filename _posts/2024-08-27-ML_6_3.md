---
layout: post
title:  "혼공 ML 6-3장"
published: true
date:   2024-08-26 04:50:00
categories: ML(MachineLearning)
permalink: /ml/6-3
---

# 6-2 주성분 분석

- 특성 = 차원
- 데이터에 있는 분산이 큰 방향을 찾는 것
- 분산이 큰 방향을 데이터로 잘 표현하는 벡터를 찾음


``` python
#pca분석 import
from sklearn.decomposition import PCA
#n_components=50: 주성분의 개수를 50개로 설정
pca = PCA(n_components=50)
pca.fit(fruits_2d)

#pca 클래스가 찾은 주성분은 components_에 저장되어 있음
print(pca.components_.shape)

#배열의 첫번째 차원이 50(주성분이 50개), 두전째 차원은 항상 원본 데이터의 특성개수와 같음
(50, 10000)

# 주성분 사진 출력
import matplotlib.pyplot as plt 
def draw_fruits(arr,ratio=1):
  n = len(arr)
  rows = int(np.ceil(n/10))
  cols = n if rows < 2 else 10  
  fig,axs = plt.subplots(rows,cols,figsize=(cols*ratio,rows*ratio),squeeze=False)
  for i in range(rows):
    for j in range(cols):
      if i*10+j < n:
        axs[i,j].imshow(arr[i*10+j],cmap='gray_r')  
        axs[i,j].axis('off')
  plt.show()

draw_fruits(pca.components_.reshape(-1,100,100))
```
![image](https://github.com/user-attachments/assets/8f03bead-f288-4016-956c-4a9cbf0e98df)

주성분을 찾았으므로 원본 데이터의 차원을 transform()을 사용해서 줄여보자
```python
#pca클래스가 찾은 주성분 배열의 크기
print(pca.components_.shape)
(50, 10000)
#transform을 사용해서 줄인 특성
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape) 
(300, 50)
```

## 원본 데이터 재구성

- invrse_transform() 메서드를 사용해서 특성을 복원할 수 있음
```python
fruits_back = pca.inverse_transform(fruits_pca)
print(fruits_back.shape)
(300, 10000)

```

## 설명된 분산
- 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값
- pca 클래스의 explained_variance_ratio_에 각 주성분의 설명된 분산 비율이 기록되어 있음
- 처번재 주성분의 설명된 분산이 가장 큼

```python
print(np.sum(pca.explained_variance_ratio_))

0.921532224084001

plt.plot(pca.explained_variance_ratio_)
plt.show()
```
![image](https://github.com/user-attachments/assets/4b58ebde-97e8-455a-9ae1-7b0b276ca7d0)

- 처음 10개의 주성분이 대부분의 분산을 표현하고 있음을 알 수 있음

## 다른 알고리즘과 함께 사용하기
- 로지스틱 회귀 모델을 사용해서 사용

``` python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

#0,1,2,의 타깃값을 100개씩 생성
target = np.array([0]*100+[1]*100+[2]*100)

#교차검증 점수 및 시간
from sklearn.model_selection import cross_validate
scores = cross_validate(lr,fruits_2d,target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

0.9966666666666667
2.1369511127471923

#pca로 축소한 fruits_pca를 사용했을 때의 교차검증 점수 및 시간
scores = cross_validate(lr,fruits_pca,target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

1.0
0.02084770202636719
```
50개의 특성만 사용했는데 정확도가 늘어나고 훈련 시간은 월씬 감소한것을 볼 수 있다

설명된 분산의 50%에 달하는 주성분을 찾도록 pca모델을 만들어 보자
``` python
pca = PCA(n_components=0.5)
pca.fit(fruits_2d)

print(pca.n_components_)
2
```
단 2개의 주성분으로 50%의 분산을 설명할 수 있다

``` python
#단 2개의 특성을 사용한 교차검증 결과
scores = cross_validate(lr,fruits_pca,target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

0.9933333333333334
0.0852806568145752
```
단 2개의 특성만 사용해도 99%의 정확도를 달성했다

또한 pca 분석을 하여 3개 이하의 차원으로 줄이면 시각화를 하기 용이하다
```python
#산점도 그리기
for label in range(0,3):
  data = fruits_pca[km.labels_==label]
  plt.scatter(data[:,0],data[:,1])
plt.legend(['apple','pineapple','banana'])
plt.show()  
```
![image](https://github.com/user-attachments/assets/efb25bb9-e16d-4818-9872-c247dc5584e4)