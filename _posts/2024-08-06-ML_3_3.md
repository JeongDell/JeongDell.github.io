---
layout: post
title:  "혼공 ML 3-3장"
published: true
date:   2024-08-06 03:50:00
categories: ML(MachineLearning)
permalink: /ml/3-3
---

# 3-3 특성 공학과 규제

## 다중회귀 

- 2개의 특성을 사용하여 학습시키면 평면을 학습한다.    

- 3개 이상의 특성을 사용하여 학습 시키면 고차원의 복잡한 모델을 표현할 수 있음

- 기존의 특성을 사용하여 새로운 특성을 뽑아낼 수도 있다(ex: 농어 길이*농어 높이)

- 이 작업을 **특성공학**이라고 한다

![image](https://github.com/user-attachments/assets/7a8f222b-e334-4cb7-b83d-309ee6b2c1f9)

## 판다스로 데이터 가져오기

```python
import pandas as pd 

df = pd.read_csv('https://bit.ly/perch_csv')

#넘파이 배열로 변환
perch_full = df.to_numpy()
print(perch_full)

```

## 사이킷런의 변환기

- 사이킷 런의 특성을 만들거나 전처리하기 위한 클래스

```python
from sklearn.preprocessing import PolynomialFeatures  

#fit 한 후 transform 가능
#PolynomialFeatures: 각 특성을 제곱한 값과 특성끼리 곱한 항을 추가
#include_bias =flase로 지정하면 출력에서 1을 지울 수 있음
#지정하지 않아도 자동으로 추가된 절편항을 무시함

poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
[[1. 2. 3. 4. 6. 9.]]
```

## 다중 회귀 모델 훈련하기

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))

0.9903183436982125
```
score 값이 아주 높게 나온 것을 알 수 있다. 

```
print(lr.score(test_poly, test_target))

0.9714559911594111
```
테스트 점수에 대한 score 값이 매우 높진 않지만 과소적합 문제는 나타나지 않았다.

degree 매개변수를 지정하여 필요항 고차항의 최대 차수를 지정할 수 있다.
> 이렇게 5차까지 지정하면 훈련 score 값에서는 1에 가깝게 나오지만 테스트 score에서는 값이 작거나 심지어는 음수가 나올 수 있다.
>> 특성의 개수를 크게 늘리면 선형 모델은 아주 강력해지지만, 훈련세트에 너무 과대적합 되므로 적합하지 않다
```python
poly= PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
```

## 규제

- 머신러닝 모델이 훈련세트를 너무 과도하게 학습하지 못하도록 하는 것

   - 모델이 훈련세트에 과대적합되지 않도록 만드는 것
   - 선형 회귀 모데르이 경우 특성에 곱해지는 계수(기울기)의 크기를 작게 만드는 일

    

![image](https://github.com/user-attachments/assets/f6d4f8bb-cb06-4641-9867-c52d081282c4)

- 특성의 스케일이 정규화 되지 않으면 곱해지는 계쑤값도 차이가 나게 된다
- 일반적으로 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 다르면 공정하게 규제할 수 없음

``` python
#standardscler 클래스를 사용하여 정규화 하기
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

## 릿지 회귀
- 계수를 제곱한 값을 기준으로 규제를 적용

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))

0.9896101671037343

print(ridge.score(test_scaled, test_target))

0.9790693977615387
```
> 테스트 점수가 훈련 세트 점수보다 낮은, 과대 적합되지 않은 모델이 만들어졌다.

릿지와 라쏘 모델을 사용할 때 **alpha** 매개변수로 규제의 강도를 조절할 수 있다.   

 alpha의 값이 클 수록 규제의 강도가 강하다. 즉 과소 적합되도록 유도한다.

 적잘한 alpha 값을 찾는 방법 중 하나는 alpha에 대한 R2 그래프를 그리는 것이다.   

 훈련 세트와 테스트 세트의 점수가 가장 가까운 값이 최적의 alpha 값이다. 

아래의 코드는 alpha 값을 0.001부터 100까지 10배씩 늘려가면서 릿지 회귀 모델을 훈련하고, 훈련세트와 테스트 세트의 점수를 파이썬 리스트에 저정한 것이다. 


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
train_score = []
test_score = []

for alpha in alpha_list:
    # 릿지 모델 생성 
    ridge = Ridge(alpha=alpha)
    # 릿지 모델 훈련
    ridge.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수 저장
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))

# 그래프 간 간격 조절을 위해 상용로그 씌우기
plt.plot(np.log10(alpha_list), train_score, label='Train score')
plt.plot(np.log10(alpha_list), test_score, label='Test score')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()
plt.show()
```
![image](https://github.com/user-attachments/assets/318bd84d-5c28-44c3-9f6d-2de01e9418ee)
> 파란색: 훈련세트그래프   
주황색: 테스트 세트 그래프

적절한 alpha의 값은 0.1임을 알 수 있다.
alpha = 0.1로 릿지 모델을 다시 만들었다. 

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))  
print(ridge.score(test_scaled, test_target))

0.9903815817570367
0.9827976465386928
```
훈련세트와 테스트 세트 모두에서 점수가 높고 과대적합과 과소적합 사이에 있는 모델이 만들어졌다.

## 라쏘 회귀
- 계수의 절댓값을 기준으로 규제를 적용한 것

```
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))  
print(lasso.score(test_scaled, test_target))

0.989789897208096
0.9800593698421883
```
>라쏘 모델도 테스트 점수가 좋으면서 과대 적합을 억제한 것을 볼 수 있다.

라쏘 모델도 alpha를 매개변수로 하여 규제의 강도를 조절할 수 있다. 

```PYTHON
train_score=[]
test_score=[]
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]   
for alpha in alpha_list:

  #라쏘 모델 생성
  lasso = Lasso(alpha=alpha, max_iter=10000)

  #라쏘 모델 훈련
  lasso.fit(train_scaled, train_target)

  #훈련 점수와 테스트 점수 저장
  train_score.append(lasso.score(train_scaled, train_target))
  test_score.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score, label='Train score')
plt.plot(np.log10(alpha_list), test_score, label='Test score')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```
![image](https://github.com/user-attachments/assets/62b782c7-d17b-4f84-8034-8c20c8bcab95)
> 파란색: 훈련세트그래프   
주황색: 테스트 세트 그래프

위 그래프에서 왼쪽은 과대적합을, 오른쪽에서 점수가 크게 떨어지는 지점은 과소적합인 부분을 나타낸다. 

최적의 값은 alpha=1 즉 10이다.

```python
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))  
print(lasso.score(test_scaled, test_target))

0.9888067471131867
0.9824470598706695
```
회귀 모델이 잘 만들어졌다. 

또한 라쏘모델은 계수 값을 0으로 만들 수 있다. 즉 유용한 특성값이 무엇인지 골라내는 용도로도 사용할 수 있다. 

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso

# Example data
X = pd.DataFrame({
    'feature1': [0.1, 0.2, 0.3],
    'feature2': [1.0, 1.1, 1.2],
    'feature3': [10, 20, 30]
})
y = np.array([1, 2, 3])

# Fit Lasso model
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

# Print the number of coefficients that are zero
print(np.sum(lasso.coef_ == 0))

# Get the names of the features with zero coefficients
zero_coef_features = X.columns[lasso.coef_ == 0]
print("Features with zero coefficients:", zero_coef_features)

```

