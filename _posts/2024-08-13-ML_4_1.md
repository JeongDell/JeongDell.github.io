---
layout: post
title:  "혼공 ML 4-1장"
published: true
date:   2024-08-13 03:50:00
categories: ML(MachineLearning)
permalink: /ml/4-1
---

# 4-1 로지스틱 회귀

- 선형 방정식을 학습하는 분류 모델


![image](https://github.com/user-attachments/assets/a66f13ce-f8e6-432f-8a8b-30af8267eadf)   

- a,b,c,d: 가중치
- 0 < z <1이 되게 하기 위해 시그모이드 함수(로지스틱 함수)를 사용

![image](https://github.com/user-attachments/assets/0e6474c9-a323-4c10-b4bb-4838b23fe48f)

이 함수를 사용하면 항상 0 < z <1 이므로 y축 값을 확률로 해석 가능함

- 음성클래스 : 시그 모이드 함수의 출력 <0.5

- 양성클래스: 시그 모이드 함수의 출력 > 0.5

## 로지스틱 회기로 이진 분류 수행하기

- 불리언 인겍싱 : 넘파이 배열은 Ture/Flase 값을 전달하여 행을 선택할 수 있음

``` PYTHON
# A/B/C/D/E로 이루어진 배열에서 A와 C만 골라내기

char_arr = np.array(['A','B','C','D','E'])
print(char_arr[[True, False, True, False, False]])
```

훈련세트에서 도미와 빙어인 행만 골라내 본다

```python
# |: OR 연산자
# train target에서 도미와 빙어만 True, 나미지는 FALSE

bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

로지스틱 회귀 모델이 위 데이터를 훈현하도록 만든다

```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

print(lr.predict(train_bream_smelt[:5]))

['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']


# predict_proba()
# train_bream_smelt에서 처음 5개 샘플의 예측확률을 출력
print(lr.predict_proba(train_bream_smelt[:5])) 

# 항상 첫번째 열이 음성 클래스(0)의 확률, 두번재 열이 양성 클래스(1)의 확률
[[0.99759855 0.00240145]
 [0.02735183 0.97264817]
 [0.99486072 0.00513928]
 [0.98584202 0.01415798]
 [0.99767269 0.00232731]]

 #Bream과 Smelt 중 무엇이 양성 클래스인지 출력
 #사이킷런은 타깃값을 알파벳 순으로 정렬하여 사용
# 일반적으로 음성/양성 순서
 print(lr.classes_)
['Bream' 'Smelt']

#로지스틱 회귀모델의 계수 확인
print(lr.coef_, lr.intercept_)

[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]

#z값 확인
#decision_frnction: 양성 클래스에 대한 z값 반환
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

#z값을 이용하여 확률 계산

from scipy.special import expit
print(expit(decisions)) 
[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
```

## 로지스틱 회기로 다중 분류 수행하기

- LogisticRegerssion 클래스는 반복적인 알고리즘을 사용
- 다중 분류를 위해 반복횟수를 증가 시켜야 함
  -  max_iter 매개변수에서 지정되어 있는 반복횟수 증가시키기
- LogisticRegerssion도 계수의 제곱을 규제함(L2 규제)
  - 매개변수 C의 값을 완화시켜야 함(C는 작을수록 규제가 커짐)


```python

lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target)) 

0.9327731092436975
0.925
```
과대 적합/과소 적합은 관측되지 않음

```python
#훈련세트의 처음 5개 데이터에 대한 예측
print(lr.predict(test_scaled[:5]))
['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

#5개 샘플에 대한 예측확률 출력
#소숫점 4번째 자리에서 반올림

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]

 # 각 열 별로 어떤 클래스인지 확인
 print(lr.classes_)
 ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
 ```

 다중 분류일 경우 선형 방정식의 모습을 확인해보자

 ```python
 print(lr.coef_.shape, lr.intercept_.shape)

 (7, 5) (7,)
 ```
 > 5개의 특성을 사용하므로 coef_배열의 열은 5개이다.
>>그렇다면 행과 intercept는 왜 7개일까?
>>> 다중 분류는 클래스마다 z값을 계산하기 때문에 7개가 나온 것이다

- 다중분류는 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다
- z값의 합이 1이 되면 된다

```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))

[[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
 [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
 [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
 [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
 [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]

 from scipy.special import softmax
 # axis=1 로 지정하여 각 행, 즉 각 샘플에 대해 소프트맥스를 계산
 # 특정 행에 대한 계산을 수행해야 할 때 axis 지정
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))  

[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
 ```