---
layout: post
title:  "혼공 ML 4-2장"
published: true
date:   2024-08-14 03:50:00
categories: ML(MachineLearning)
permalink: /ml/4-2
---

# 4-2 확률적 경사 하강법

- 훈련세트를 여러번 사용하여 조금씩 산 아래에 있는 최적의 장소로 이동하는 알고리즘

- 에보크: 훈련세트를 한번 다 사용하는 과정

- 손실함수(산)
> 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준   

> 손실함수는 미분가능해야 한다
>> 분류에서는 정답을 맞추지 못하는 것

![image](https://github.com/user-attachments/assets/362c9dd1-7798-43d8-bb55-4581a21abae4)

## 로지스틱 손실 함수

- 예측과 타깃이 멀어질수록 작은 값이 나오도록 계산

$$
예측 \times 정답(타깃) -> -(예측\times정답)   $$

정답이 0이라면
$$예측 = 1- 실제 예측 값$$

![image](https://github.com/user-attachments/assets/1f2378bc-8f73-4350-8501-35d85dfd391f)

- 이진분류에서는 이진 크로스엔트로피 손실함수(로지스틱 손실함수)를 사용

- 다중 분류에서는 크로스엔트로피 손실 함수 사용

# SGD Classifier

```python
from sklearn.linear_model import SGDClassifier

#mat_tier로 반복횟수 지정
sc = SGDClassifier(loss='log_loss', max_iter=20, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))

print(sc.score(test_scaled, test_target))

0.6554621848739496
0.65

#훈련세트 한번 더 학습
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))   

0.865546218487395
0.9
```

>훈련세트를 여러번 학습, 즉 에포크를 여러번 실행하면 정확도가 향상되는 것을 볼 수 있다


## 에포크와 과대/과소 적합

![image](https://github.com/user-attachments/assets/d6a8f201-1e65-40da-a5af-cc840687568c)

- 그림처럼 에포크가 너무 적으면 과소적합이 발생하고, 너무 많으면 과대적합이 발생한다
- 과대적합이 시작하기 전에 훈련을 멈추는 것을 조기 종료라고 한다




