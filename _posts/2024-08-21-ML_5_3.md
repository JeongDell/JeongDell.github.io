---
layout: post
title:  "혼공 ML 5-3장"
published: true
date:   2024-08-20 03:50:00
categories: ML(MachineLearning)
permalink: /ml/5-3
---

# 5-3 트리의 앙상블

- 앙상블 학습: 정형 데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘

## 랜덤 포레스트

- 결정 트리를 랜덤하게 만들어 결정트리의 숲을 만들고, 각 결정 트리의 예측을 사용해 최종 예측을 만듬
- 기본적으로 100개의 결정 트리를 훈련함
- 랜덤하게 샘플을 선택하고, 특성을 사용하기 때문에 과대적합을 막고, 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있음

### 랜덤 포레스트의 특징

1. 부트스트랩 샘플
- 각 트리를 훈련하기 위한 데이터를 만들 때 한 샘플이 중복되어 추출 될 수 있음
- 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 룬련 데이터를 만듬
- 훈려 크기의 크기와 같은 크기로 수트스트랩 샘플을 만듦

2. 특성 선별

- 전체 특성중 일부 특성을 무작위로 고르고 최선의 분할을 찾음
- 전체 특성 개수의 제곱근 만큼의 특성을 선택함

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine = pd.read_csv('https://bit.ly/wine-date')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

from sklearn.model_selection import cross_validate

#랜덤 포레스트 import
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)

#retrin_train_score=True로 지정하여 훈련세트에 대한 점수도 같이 반환
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

# 각 특성에 대한 중요도 출력
# 랜덤하게 특성을 추출해서 결정트리를 훈련하기 때문에 좀 다양한 특성을 학습할 수 있음
#과대 적합을 줄이고 일반화 성능을 높이는데 도움이 됨
rf.fit(train_input, train_target)
print(rf.feature_importances_)
[50]

[0.23167441 0.50039841 0.26792718]

#OOB 샘플: 훈련데이터에 포함되지 못한 데이터를 기반으로 결정트리를 평가할 수 있는 샘플
#oob_score=True를 통해 점수를 나타낼 수 있음
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
0.8934000384837406
```

## 엑스트라 트리

- 기본적으로 100개의 결정 트리를 훈련

- 결정 트리가 제공하는 대부분의 매개변수를 지원함

- 전체 특성 중 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용

- 노드를 분할 할 때 무작위로 분할함

- 성능은 낮아지지만 과대적합을 막고 검증 세트의 점수를 높일 수 있음

- 빠른 계산속도가 큰 장점

```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

0.9974503966084433 0.8887848893166506
```

## 그레이디언트 부스팅

- 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블하는 방법

- 경사하강법을 이용하여 트리를 앙상블에 추가함

- 분류에서는 로지스틱 손실 함수를, 회귀에서는 평균 제곱 오차 함수를 사용함

- 학습률 매개변수로 내려가는 속도를 조절함


```python
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

subsample 매개변수
- 기본 값: 1.0
- 1보다 작으면 훈련 세트의 일부분을 사용함

## 히스토그램 기반 그레이디언트 부스팅
- 입력 특성을 256개의 구간으로 나눔

- 256개의 구간 중에서 하나를 떼어 놓고 누란된 값을 위해서 사용함
   - 입력에 누락된 특성이 있어도 전처리할 필요가 없음


```python

from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoost
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)  
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```