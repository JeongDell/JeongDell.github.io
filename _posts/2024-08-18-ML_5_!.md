---
layout: post
title:  "혼공 ML 5-1장"
published: true
date:   2024-08-18 03:50:00
categories: ML(MachineLearning)
permalink: /ml/5-1
---

# 5-1 결정 트리

- YES/NO로 데이터를 계속해서 분류해서 정답을 맞춰가는 과정

- 사이킷런의 DecisionTreeClassifier 사용하여 훈련

``` python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![image](https://github.com/user-attachments/assets/8843e386-1be2-42b3-b370-804750498e7a)

위 그림에서 맨 위의 노드를 루트 노드, 맨 아래 끝의 노드를 리프 노드라고 한다

``` python
# plot_tree()에서 트리의 깊이 제한해서 출력하기
#depth: 깊이
#filled = True: 클래스에 맞게 색칠, 특정 클래스의 비율이 높아지면 점점 색깔이 진해짐
#featured_names: 특성의 이름 전달
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![image](https://github.com/user-attachments/assets/9fe43afb-a1df-45a6-be6e-23764598ef40)

각 노드에는 테스트 조건, 불순도, 총 샘플 수, 클래스별 샘플 수에 대한 데이터가 주어져 있다.
클래스 별 샘플 수는 음성/양성 순서이다.

![image](https://github.com/user-attachments/assets/90f5dbb4-4bce-49bc-900e-ed0fd823c126)

>  결정트리에서의 예측은 리프 노드에서 가장 많은 클래스가 예측값이 된다.

## 불순도

- gini는 지니 불순도를 의미한다. 
- DecisonTreeeClassifier 클래스의 criterion 매개변수의 기본값이 gini이다
- criterion 매개변수는 데이터를 분할할 기준을 정하기 위해 사용한다
- 지니 불순도는 아래의 식으로 계산된다
- 100개의 샘플이 있는 어떤 노드에서 클래스 비율이 정확히 1/2라면 지니 불순도는 0.5가 되고, 모두 같은 클래스라면 지니불순도는 0이 된다

![image](https://github.com/user-attachments/assets/e657f67f-5394-4c81-8731-a1b3283de78e)


- 결정트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킨다
![image](https://github.com/user-attachments/assets/d868c805-923e-46b0-ae4a-64a980499373)

> 부모 노드와 자식 노드 사이의 불순도 차이를 정보 이득이라고 부르고, 결정트리모델은 정보이득이 최대가 되도록 데이터를 나눈다

>> gini 외에도 criterion = 'entropy'를 활용하여 엔트로피 불순도를 사용할 수 있다.

## 가지치기

- 결정 트리에서는 결정트리의 최대 깊이를 지정하여 과대적합을 방지할 수 있다.

- 또한 결정트리를 사용할 때는 스케일이 결정 트리 알고리즘에 아무런 영향을 미치지 않기 때문에 표준화 전처리를 하지 않아도 된다.
